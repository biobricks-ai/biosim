{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/thomas/miniconda3/envs/deepchem/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2022-09-25 21:58:58.095936: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-09-25 21:58:58.227695: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2022-09-25 21:58:58.746393: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2022-09-25 21:58:58.746494: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2022-09-25 21:58:58.746506: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "Downloading: 100%|██████████| 327/327 [00:00<00:00, 380kB/s]\n",
      "Downloading: 100%|██████████| 8.14k/8.14k [00:00<00:00, 8.74MB/s]\n",
      "Downloading: 0.00B [00:00, ?B/s]\n",
      "Downloading: 100%|██████████| 112/112 [00:00<00:00, 156kB/s]\n",
      "Downloading: 100%|██████████| 515/515 [00:00<00:00, 612kB/s]\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'RobertaTokenizer'. \n",
      "The class this function is called from is 'RobertaFeaturizer'.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "/home/thomas/miniconda3/envs/deepchem/lib/python3.9/site-packages/deepchem/feat/base_classes.py:58: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  return np.asarray(features)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[list([12, 16, 25, 20, 15, 17, 22, 19, 18, 15, 21, 15, 17, 25, 15, 25, 21, 16, 18, 25, 17, 16, 18, 15, 20, 22, 19, 13]),\n",
       "        list([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])],\n",
       "       [list([12, 16, 16, 17, 22, 19, 18, 23, 20, 16, 23, 17, 16, 17, 16, 18, 22, 19, 18, 16, 17, 19, 18, 16, 20, 19, 13]),\n",
       "        list([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])]],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import deepchem as dc\n",
    "import numpy as np\n",
    "from deepchem.feat import RobertaFeaturizer\n",
    "train_smiles = [\n",
    "        'Cc1cccc(N2CCN(C(=O)C34CC5CC(CC(C5)C3)C4)CC2)c1C',\n",
    "        'Cn1ccnc1SCC(=O)Nc1ccc(Oc2ccccc2)cc1',\n",
    "        'COc1cc2c(cc1NC(=O)CN1C(=O)NC3(CCc4ccccc43)C1=O)oc1ccccc12',\n",
    "        'O=C1/C(=C/NC2CCS(=O)(=O)C2)c2ccccc2C(=O)N1c1ccccc1',\n",
    "        'NC(=O)NC(Cc1ccccc1)C(=O)O', 'CCn1c(CSc2nccn2C)nc2cc(C(=O)O)ccc21',\n",
    "        'CCc1cccc2c1NC(=O)C21C2C(=O)N(Cc3ccccc3)C(=O)C2C2CCCN21',\n",
    "        'COc1ccc(C2C(C(=O)NCc3ccccc3)=C(C)N=C3N=CNN32)cc1OC',\n",
    "        'CCCc1cc(=O)nc(SCC(=O)N(CC(C)C)C2CCS(=O)(=O)C2)[nH]1',\n",
    "        'CCn1cnc2c1c(=O)n(CC(=O)Nc1cc(C)on1)c(=O)n2Cc1ccccc1'\n",
    "    ]\n",
    "tokens = set()\n",
    "for s in train_smiles:\n",
    "  tokens = tokens.union(set(c for c in s))\n",
    "tokens = sorted(list(tokens))\n",
    "max_length = max(len(s) for s in train_smiles) + 1\n",
    "s = dc.models.seqtoseq.AspuruGuzikAutoEncoder(tokens, max_length)\n",
    "\n",
    "def generate_sequences(smiles, epochs):\n",
    "  for i in range(epochs):\n",
    "    for s in smiles:\n",
    "      yield (s, s)\n",
    "\n",
    "s.fit_sequences(generate_sequences(train_smiles, 100))\n",
    "\n",
    "# Test it out.\n",
    "pred1 = s.predict_from_sequences(train_smiles, beam_width=1)\n",
    "pred4 = s.predict_from_sequences(train_smiles, beam_width=4)\n",
    "embeddings = s.predict_embeddings(train_smiles)\n",
    "pred1e = s.predict_from_embeddings(embeddings, beam_width=1)\n",
    "pred4e = s.predict_from_embeddings(embeddings, beam_width=4)\n",
    "\n",
    "for i in range(len(train_smiles)):\n",
    "  assert pred1[i] == pred1e[i]\n",
    "  assert pred4[i] == pred4e[i]\n",
    "\n",
    "@pytest.mark.tensorflow\n",
    "def test_variational(self):\n",
    "\"\"\"Test using a SeqToSeq model as a variational autoenconder.\"\"\"\n",
    "\n",
    "sequence_length = 10\n",
    "tokens = list(range(10))\n",
    "s = dc.models.SeqToSeq(\n",
    "    tokens,\n",
    "    tokens,\n",
    "    sequence_length,\n",
    "    encoder_layers=2,\n",
    "    decoder_layers=2,\n",
    "    embedding_dimension=128,\n",
    "    learning_rate=0.01,\n",
    "    variational=True)\n",
    "\n",
    "# Actually training a VAE takes far too long for a unit test.  Just run a\n",
    "# few steps of training to make sure nothing crashes, then check that the\n",
    "# results are at least internally consistent.\n",
    "\n",
    "s.fit_sequences(generate_sequences(sequence_length, 1000))\n",
    "for sequence, target in generate_sequences(sequence_length, 10):\n",
    "  pred1 = s.predict_from_sequences([sequence], beam_width=1)\n",
    "  embedding = s.predict_embeddings([sequence])\n",
    "  assert pred1 == s.predict_from_embeddings(embedding, beam_width=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deepchem/deepchem/feat/roberta_tokenizer.py "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('deepchem')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c0ebaec5eb0f00878ef564b7bd9710d75c652634558a3a169fb7bda27e637042"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
